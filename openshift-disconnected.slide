Openshift in a Disconnected world
21 Sep 2021
Tags: Openshift, Disconnected, IPv6

Juan Manuel Parrilla
Principal Software Engineer, Red Hat
jparrill@redhat.com

Mario Vazquez Cebrian
Senior Software Engineer, Red Hat
mavazque@redhat.com

* Disconnected environments are dark places!

.image assets/img/zelda.jpg 400 600

You better check the documentation before you start dealing with a real one!
.background assets/img/background.png

########

* Index

- Pre-requisites

    - A Helper node
    - Helper Node Sysctl Settings
    - Additional Services: DHCP, DNS, Container Registry, etc.
    - OpenShift Release Image Sync
    - Red Hat CoreOS Image Sync
    - Networking in-place
    - A valid install-config file
    - Disconnected OLM (CatalogSources and ICSPs)

- OCP Deployment
    
    - UPI/IPI
    - Deployment Customization

- Openshift and Operators Life-cycle

.background assets/img/background.png

########
* Pre-requisites

.background assets/img/background.png

########
* Pre-requisites: Helper Node

Basically almost all pre-requisites need to be configured on the *helper* *node*. We call _helper_ _node_ to a node which provides the basic services to our IPv6 environment, like DNS, DHCP, HTTPD, etc... So let's create a list of services necessary to make this work:

.image assets/img/pre-reqs.png 370 780

.background assets/img/background.png

########
* Pre-requisites: Helper Node
We can classify them as you see in the diagram:

- Mandatory
- Mandatory for Disconnected
- Mandatory for IPv6
- Optional Components

.background assets/img/background.png

########
* Pre-requisites: Helper Node

- As *DNS* and *DHCP* servers we will use `DNSMasq` software since it can provide both services
- For *NTP* we will use `Chrony`
- For *Internal* *Registry* we will use a podman container with the docker-registry image
- For the *HTTP* server we will use `HTTPD`
- For RA's and route spreading on the IPv6 environment we will use RaDVD
- To allow access to the Openshift Console we will have an *HAProxy* that makes the IPv4>IPv6 translation

These are the basic components on the Helper Node. 

Keep in mind that if you already have some of these services running on your current infra you can leverage those.

.background assets/img/background.png

########
* Pre-requisites: Sysctl Settings

Some sysctl settings are required for the helper node services to work properly:

    # Enable IPv6 Forwading for all interfaces
    net.ipv6.conf.all.forwarding=1
    
    # Accept RA's even if forwarding is enabled
    net.ipv6.conf.all.accept_ra=2
    
    # Reverse Path not validated
    net.ipv4.conf.all.rp_filter=0
    
    # Disable IPv6 Loopback config
    net.ipv6.conf.lo.disable_ipv6=0

.link https://sysctl-explorer.net

.background assets/img/background.png
########
* Pre-requisites: Additional Services
Firstly we need a couple of mandatory services, they will be DNS and NTP.

- DNS needs to at least resolve the main OpenShift DNS Records (API, API-INT and Ingress Wildcard)
- OpenShift node DNS records are not required for IPI deployments but will be required for UPI deployments
- The Helper Node should have a valid DNS record so we can reference the container registry using that DNS record
- NTP is also mandatory in a multinode deployment, we usually put a server on the Helper node and allow time syncrhonization from the network we OCP nodes are located
- Pull Secret is required in order to get access to the images for the installation
.background assets/img/background.png

########
* Pre-requisites: OpenShift Release Image Sync

To start the image synchronization for the OCP installation it's quite easy, we will just need to :

- Create a TLS Certificate for our registry with a proper subject name
.code assets/src/cert-maker.sh /^host/,/htpasswd/
.background assets/img/background.png

########
* Pre-requisites: OpenShift Release Image Sync
- Start the Image Registry in local
.code assets/src/start-registry.sh /^podman/,/docker/

*Note*: If you plan to use disconnected OLM, make sure the `schema1` compatibility is enabled on the docker-registry config file

.background assets/img/background.png

########
* Pre-requisites: OpenShift Release Image Sync

- Download the official oc client and fill the script vairables 
.code assets/src/release-sync-full.sh /^# Variables/,/NO/

- Now execute the synchronization:
.code assets/src/release-sync-full.sh /^*oc adm/,/--to-release-image/

.background assets/img/background.png

########
* Pre-requisites: Red Hat CoreOS Image Sync

Once the container images have been synced, we need to download the RHCOS images for the OCP deployment, in order to do that we have a script that extracts the information from the Openshift release:

- Extract the Openshift-Baremetal Installer from the mirrored release

.code assets/src/release-sync-full.sh /^*Downloading IPI/,/--to ./
This will extract the IPI installer binary from the release you just mirrored.
Ensure you move the binary to a folder within the $PATH and change the execution permissions flag on the binary.
.background assets/img/background.png

########
* Pre-requisites: Red Hat CoreOS Image Sync
- Download the RHCOS Images

We use the installer binary to extract the proper URLs for the mirrored release, this is a sample about how to extract them but you can find all needed resources in the `assets/src/release-sync-full.sh` script.

    # RHCOS Version
    openshift-baremetal-install coreos print-stream-json \
    | jq -r '.["architectures"]["x86_64"]["artifacts"]["metal"]["release"]'

    # RHCOS ISO URI
    openshift-baremetal-install coreos print-stream-json \
    | jq -r '.["architectures"]["x86_64"]["artifacts"]["metal"]["formats"]["iso"]["disk"]["location"]'

Above is just a sample, we will see which images are required to be mirrored and where you should place them in the next slides.

.background assets/img/background.png
########

* Pre-requisites: Red Hat CoreOS Image Sync

Summarizing you will need these elements to place them into the `install-config.yaml` file:

- QEMU URI
- QEMU SHA256 Uncompressed
- OPENSTACK URI
- OPENSTACK SHA256 Compressed

In the `install-config.yaml` format will be something like:

    bootstrapOSImage: http://[2100:52:1302::1]/rhcos-qemu.x86_64.qcow2.gz?sha256=17868a1963ae2eae25...
    clusterOSImage: http://[2100:52:1302::1]/rhcos-openstack.x86_64.qcow2.gz?sha256=6ab5c6413f27527...

The IPv6 with the image name is our internal httpd server hosting the images
The `sha256=...` is the hash as reported by the openshift-baremetal-install binary.

.background assets/img/background.png

########
* Pre-requisites: Networking 

The networking part is a bit tricky, you need to know how IPv6 works in a generic network with the RA/NA/ND and so on, once you know that, you need apply it to your network checking with your network admin if some capabilities are disabled or not, let's get into it.

Important points to discuss:

- DNSMasq and RAs
- IPv6 and SLAAC Addresses
- PXE vs VirtualMedia
- BootMode: UEFI vs Legacy


.background assets/img/background.png
########

* DNSMasq and RAs 

- Configure your network to send the RAs from RaDVD instead of sending RAs on DNSMasq (DNSMasq is sometimes too slow sending RAs and that causes installation issues)
- `AdvOnLink` `on` and `AdvRouterAddr` `on` are mandatory options if not, NetworManager will get confused
- You can use *ndptool* to to debug the RAs sent over the network

    ndptool mon -i 'baremetal.154' -t ra

.background assets/img/background.png
########

* IPv6 and SLAAC Addresses 

- SLAAC addresses are the devil if you are not the network admin 
- You can check if those are enabled or not with `ndptool`
.image assets/img/slaac_trace.png 200 950
- Main issue is that your OCP nodes will get the SLAAC address configured before their DHCP address and Kubelet will use the SLAAC which will cause issues
- If SLAAC addresses are enabled, routes and DNS resolution will be affected

.background assets/img/background.png
########

* PXE vs VirtualMedia 

- PXE will involve a more complex network config, and usually PXE is not always allowed in all infrastructures
- PXE requires an additional L2 network where provisioning services will run (including PXE)
- VirtualMedia is the recommended way providing your hardware has redfish support on its BMC (Dell and HP recent hardware is recommended)
- VirtualMedia requires the installer to reach the BMC addresses, no additional network will be required

.background assets/img/background.png
########

* UEFI vs Legacy 

- There are a couple scenarios where UEFI boot will be required
- If you plan to use NVMe drives
- If your BMC requires UEFI for PXEBooting on IPv6
- At this point UEFI secure boot should be disabled


.background assets/img/background.png
########

* The Install Config file

.background assets/img/background.png
########

* The Install Config file

If you are deploying Openshift this is where you define the desired cluster configuration, let's review some of the required parameters
.code assets/src/install-config.yaml /^networking/,/baremetal/

.background assets/img/background.png
########

* The Install Config file

.code assets/src/install-config.yaml /^platform/,/hardwareProfile/

.background assets/img/background.png
########

* The Install Config file

.code assets/src/install-config.yaml /^imageContentSources:/,/source/
.code assets/src/install-config.yaml /^additionalTrustBundle:/,/  BQAwgYg/
.code assets/src/install-config.yaml /^pullSecret/,/ssh-rsa/

.background assets/img/background.png
########

* CatalogSources and ICSPs

.background assets/img/background.png
########

* CatalogSources

A CatalogSource is a Kubernetes object used by the OLM (Operator Lifecycle Manager) to consume PackageManifests included on that source.

- CatalogSources are namespaced resources and are usually created in the openshift-marketplace namespace
.image assets/img/catalogsource.png 80 1000
.code assets/src/catalogsource.yaml /name/,/interval/

.background assets/img/background.png
########

* CatalogSources

The CatalogSource points to an index image which contains a serie of objects that OLM parses as `PackageManifests`

- One CatalogSouce could contain 1 to N PackageManifest
- When creating a CatalogSource you can decide which PackageManifest will be included
- OCP has 4 CatalogSources included by default pointing to Red Hat registries
- You can disable the default CatalogSources in case of disconnected deployments
- The index image (consumed by the CatalogSource) contains a DB with the data for every PackageManifests it includes

.image assets/img/packagemanifest.png

.background assets/img/background.png
########

* CatalogSources

- If you wanna inspect the content of an index, just run it on podman as you see
.image assets/img/cs-serve.png 50 950

- Then in other terminal use `gprcurl` to check the content:
.image assets/img/cs-packages.png

You can use `opm` and tool to create you own indexImages and then push it to your internal registry to be consumed for your own CatalogSources.

.background assets/img/background.png
########

* CatalogSources

- Create the Index Image locally
    opm index prune --from-index registry.redhat.io/redhat/redhat-operator-index:v4.8 \
        --packages advanced-cluster-management,local-storage-operator... \
        --tag bm-cluster-1-hyper.e2e.bos.redhat.com:5000/olm-index/redhat-operator-index:v4.8
    
- Push Index image to your internal Registry
    podman push --tls-verify=false --authfile pull_secret.json \
        bm-cluster-1-hyper.e2e.bos.redhat.com:5000/olm-index/redhat-operator-index:v4.8 

- Sync the container images listed on your index image from the source registry (RedHat) to your internal Registry
    oc adm catalog mirror --registry-config=pull_secret.json \ 
        bm-cluster-1-hyper.e2e.bos.redhat.com:5000/olm-index/redhat-operator-index:v4.8 \
        bm-cluster-1-hyper.e2e.bos.redhat.com:5000/olm 

.background assets/img/background.png
########

* The ICSPs

Stands for ImageContentSourcePolicy and the concept behind this object is easy to understand

- It works at runtime level
- When crio/podman need to pull an image, they will check the `/etc/containers/registries.conf` file
- If the requested image has a valid mirror entry on that file, the pull will be done from the mirror in first step

ICSP it's a K8s object that create these mirror entries on the OCP nodes.

The controller that manage this object is the MachineConfig Operator, so make sure you don't have it in degraded state

    oc get mcp

.background assets/img/background.png
########

* The ICSPs

This is how an ICSP looks like
.code assets/src/icsp.yaml /apiVersion/,/source/

And the resulting configuration in the `/etc/containers/registries.conf` file
.code assets/src/registries.conf /registry/,/bm-cluster-1-hyper.e2e.bos.redhat.com:5000/

.background assets/img/background.png
########

* The ICSPs

But wait, there are a couple things you still need to know:

- If you have the MCO in a degraded state ICSPs will not be applied
- If you modify the `registries.conf` file manually in the OCP Nodes the MCO will enter on degraded state
- If you decide that you want to modify the configuration file manually, you will need to restart kubelet and crio after modify this file for the changes to take effect
- The flag `mirror-by-digest-only` `=` `true` will be exposed on the API on 4.9 and you will be able the modify this directly on the ICSP 
- You should not include tags/digests on the ICSP, just the registry + repo namespace
- You can save some lines puting the namespace instead the whole image


.background assets/img/background.png
########

* Openshift Deployment


.background assets/img/background.png
########

* Openshift Deployment

There are 2  main types of OpenShift deployments, UPI and IPI. Both of them have minimum requirements that requires the user to configure the environment where OCP will be deployed, here we are describing IPI:

- Create the proper DNS and DHCP entries
- Helper node configuration like networking, sysctl parameters, etc...
- Client and Installer extracted from the mirrored OCP Release
- Red Hat CoreOS Images mirrored (Qemu and Openstack ones)
- Prepare BareMetalHost (BMC config if the deployment will be on BM)

From this point we can talk about the specific needs

.background assets/img/background.png
########

* UPI: User Provided Infrastructure

The most flexible way and also the most challenging one. It has some caveats like, if you don't configure a cloud provider you will not be able to automatically create more workers in the cluster and get them deployed automatically, you need to add them manually.

The user is responsible of:

- Create the ISOs required for booting the Bootstrap and OpenShift Nodes.
- If you plan to use PXE you need to create the PXE configs and extract the ISOs.
- Bootstrap node creation (This node will bootstrap the deployment creating a temporary control plane with some basic services like ETCD, CoreDNS, LB...)
- OCP nodes creation. When the bootstrap node is running the temporary control plan, the user will need to boot the nodes with a VirtualCD/PXE and the nodes will try to get their ignition configuration from the MachineConfigServer running on the bootstrap node and they will eventually form an ETCD Cluster.

.background assets/img/background.png
########

* UPI: User Provided Infrastructure

- The nodes form an ETCD cluster with the Bootstrap node and then they will continue the deployment process of OpenShift.
- When you see a logtrace on the Bootstrap node that says something like "Bootstrap process finished", you can delete/poweroff the node.
- The 3 control plane nodes will finish the deployment by themselves.
- Once the installation has finished you can use the Kubeconfig generated by the installer to access your cluster.

Here you can find many more detail about how to proceed

.link https://cloud.redhat.com/blog/ask-an-openshift-admin-office-hour-installation-methods-redux Installation Methods Redux

.background assets/img/background.png
########

* IPI: Installer Provided Infrastructure

User will need to download the installer binary and the `oc` client, the best way to do this, is extracting it from the mirrored release as we saw before.

This method is one of the easiest ways to deploy Openshift.

- You need to provide a Libvirtd URL to allow the Installer to create the Bootstrap VM (Only for BareMetal deployments)
- You need to provide the provider information and credentials on the `install-config.yaml` file, like RHEV, OpenStack, AWS, Azure, GCP, vSphere or even Baremetal
- If the deployment is disconnected you need to add some extra manifests to the deployment folder, the installer will take care get them inserted on the multiple ISOs and that will be reflected on the Ignition files that the ISOs have inside.

.background assets/img/background.png
########

* IPI: Installer Provided Infrastructure

Using IPI is just as easy as executing a small script like this one:

.code assets/src/deploy.sh

- We copy the install-config file inside the folder we created because during the initial part of the deployment, that one is moved and transformed in a different folder.
- We generate the manifests first, because this allow us to add some manifests like ICSPs or maybe MachineConfigs that do something at deployment time (like configuring Chronyd).

.background assets/img/background.png
########

* IPI: Installer Provided Infrastructure

- After that, the deployment is started using the last command on debug mode to follow every step (debug mode is not required but will help if something goes wild).
- At some point the Bootstrap node will be created and the installer will boot the OCP Nodes using different APIs (depending on which provider is configured on the install-config).
- The bootstrap node will be deleted automatically by the Installer when it's not needed anymore.
- Once the deployment finishes you will get a message with the `kube:admin` user and password + the location of the `kubeconfig` file and the URL to access the OpenShift Console.

- If you hit some installer timeout you can still wait a bit more using this command:
    
    openshift-baremetal-install --dir $CLUSTER --log-level debug wait-for install-complete

.background assets/img/background.png
########

* Alternative Methods

There are some alternative methods to have a Cluster, you can use the web UI on the SaaS located on cloud.redhat.com which provide a couple of more methods, but the most easy one is using the Infrastructure Operator that gives you a quick form to fill, and then you just need to download an ISO and boot your nodes with it. The installer will do the rest.

You have a blogpost from one of our team members: 

.link https://cloud.redhat.com/blog/using-the-openshift-assisted-installer-service-to-deploy-an-openshift-cluster-on-metal-and-vsphere OCP Cluster on vsphere using Assisted Installer

The method using the SaaS also includes the capability to deploy an *SNO* *(Single* *Node* *Openshift)* which the native install methods are not capable yet to deploy (But they will support that in the near future, so stay tunned!)

And the last one, for challenge seekers only, you can use KCli, more info here (*Note*: KCli is not supported by Red Hat, is a community tool):

.link https://kcli.readthedocs.io/en/latest/#deploying-kubernetes-openshift-clusters-and-applications-on-top Deploy Openshift 4 using Kcli

.background assets/img/background.png
########

* Questions?

.background assets/img/background.png
########

* Thank you 

.background assets/img/background.png
########
